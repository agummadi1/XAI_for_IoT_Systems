# -*- coding: utf-8 -*-
"""Graph fix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KRu3hQwJqTQMJ4kQTPE9j4VNRnYfMjKa

CEM Feature Importance
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# List of 9 CSV files
csv_files = [
    'device1_top_20_features.csv',
    'device2_top_20_features.csv',
    'device3_top_20_features.csv',
    'device4_top_20_features.csv',
    'device5_top_20_features.csv',
    'device6_top_20_features.csv',
    'device7_top_20_features.csv',
    'device8_top_20_features.csv',
    'device9_top_20_features.csv'
]

# Loop through each CSV file
for i, csv_file in enumerate(csv_files):
    print(f"Processing file {i + 1} of 9: {csv_file}")

    # Load dataset from the current CSV file
    data = pd.read_csv(csv_file)

    # Separate features and labels
    X = data.drop(columns=['label'])
    y = data['label']

    # Define models
    random_forest_model = RandomForestClassifier()
    decision_tree_model = DecisionTreeClassifier()
    ada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100)

    # Fit models to the data
    random_forest_model.fit(X, y)
    decision_tree_model.fit(X, y)
    ada_model.fit(X, y)

    # Get the baseline accuracy for each model
    baseline_accuracy_rf = random_forest_model.score(X, y)
    baseline_accuracy_dt = decision_tree_model.score(X, y)
    baseline_accuracy_ada = ada_model.score(X, y)

    # Initialize lists to store feature importances
    rf_feature_importances = []
    dt_feature_importances = []
    ada_feature_importances = []

    # Iterate through each feature to assess its importance using CEM
    for feature in X.columns:
        # Create a perturbed dataset with the feature shuffled or modified
        X_perturbed = X.copy()
        X_perturbed[feature] = np.random.permutation(X_perturbed[feature])

        # Calculate the accuracy change due to the perturbation for Random Forest
        perturbed_accuracy_rf = random_forest_model.score(X_perturbed, y)
        rf_feature_importance = baseline_accuracy_rf - perturbed_accuracy_rf
        rf_feature_importances.append(rf_feature_importance)

        # Calculate the accuracy change due to the perturbation for Decision Tree
        perturbed_accuracy_dt = decision_tree_model.score(X_perturbed, y)
        dt_feature_importance = baseline_accuracy_dt - perturbed_accuracy_dt
        dt_feature_importances.append(dt_feature_importance)

        # Calculate the accuracy change due to the perturbation for AdaBoost
        perturbed_accuracy_ada = ada_model.score(X_perturbed, y)
        ada_feature_importance = baseline_accuracy_ada - perturbed_accuracy_ada
        ada_feature_importances.append(ada_feature_importance)

    # Plot and save CEM-based feature importances for all models
    def save_feature_importance_plot(feature_importances, title, filename):
        plt.figure(figsize=(8, 6))
        plt.bar(range(len(X.columns)), feature_importances)
        plt.xticks(range(len(X.columns)), X.columns, rotation=90)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

    save_feature_importance_plot(rf_feature_importances, f'd{i + 1} Random Forest CEM Feature Importances', f'd{i + 1}_random_forest_cem_feature_importance.jpg')
    save_feature_importance_plot(dt_feature_importances, f'd{i + 1} Decision Tree CEM Feature Importances', f'd{i + 1}_decision_tree_cem_feature_importance.jpg')
    save_feature_importance_plot(ada_feature_importances, f'd{i + 1} AdaBoost CEM Feature Importances', f'd{i + 1}_ada_cem_feature_importance.jpg')

"""LOCO Feature Importance"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# List of 9 CSV files
csv_files = [
    'device1_top_20_features.csv',
    'device2_top_20_features.csv',
    'device3_top_20_features.csv',
    'device4_top_20_features.csv',
    'device5_top_20_features.csv',
    'device6_top_20_features.csv',
    'device7_top_20_features.csv',
    'device8_top_20_features.csv',
    'device9_top_20_features.csv'
]

# Loop through each CSV file
for i, csv_file in enumerate(csv_files):
    print(f"Processing file {i + 1} of 9: {csv_file}")

    # Load dataset from the current CSV file
    data = pd.read_csv(csv_file)

    # Separate features and labels
    X = data.drop(columns=['label'])
    y = data['label']

    # Define models
    random_forest_model = RandomForestClassifier()
    decision_tree_model = DecisionTreeClassifier()
    ada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100)

    # Fit models to the data
    random_forest_model.fit(X, y)
    decision_tree_model.fit(X, y)
    ada_model.fit(X, y)

    # Get the baseline accuracy for each model
    baseline_accuracy_rf = random_forest_model.score(X, y)
    baseline_accuracy_dt = decision_tree_model.score(X, y)
    baseline_accuracy_ada = ada_model.score(X, y)

    # Initialize lists to store feature importances
    rf_feature_importances = []
    dt_feature_importances = []
    ada_feature_importances = []

    # Iterate through each feature to assess its importance using LOCO
    for feature in X.columns:
        # Leave out the current feature
        X_loco = X.drop(columns=[feature])

        # Calculate LOCO accuracy for Random Forest
        random_forest_model.fit(X_loco, y)
        loo_accuracy_rf = random_forest_model.score(X_loco, y)
        rf_feature_importance = baseline_accuracy_rf - loo_accuracy_rf
        rf_feature_importances.append(rf_feature_importance)

        # Calculate LOCO accuracy for Decision Tree
        decision_tree_model.fit(X_loco, y)
        loo_accuracy_dt = decision_tree_model.score(X_loco, y)
        dt_feature_importance = baseline_accuracy_dt - loo_accuracy_dt
        dt_feature_importances.append(dt_feature_importance)

        # Calculate LOCO accuracy for AdaBoost
        ada_model.fit(X_loco, y)
        loo_accuracy_ada = ada_model.score(X_loco, y)
        ada_feature_importance = baseline_accuracy_ada - loo_accuracy_ada
        ada_feature_importances.append(ada_feature_importance)

    # Plot and save LOCO-based feature importances for all models
    def save_feature_importance_plot(feature_importances, title, filename):
        plt.figure(figsize=(8, 6))
        plt.bar(range(len(X.columns)), feature_importances)
        plt.xticks(range(len(X.columns)), X.columns, rotation=90)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

    save_feature_importance_plot(rf_feature_importances, f'd{i + 1} Random Forest LOCO Feature Importances', f'd{i + 1}_random_forest_loco_feature_importance.jpg')
    save_feature_importance_plot(dt_feature_importances, f'd{i + 1} Decision Tree LOCO Feature Importances', f'd{i + 1}d9_decision_tree_loco_feature_importance.jpg')
    save_feature_importance_plot(ada_feature_importances, f'd{i + 1} AdaBoost LOCO Feature Importances', f'd{i + 1}_ada_loco_feature_importance.jpg')

"""PFI Feature Importance"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.inspection import permutation_importance
import numpy as np

# List of 9 CSV files
csv_files = [
    'device1_top_20_features.csv',
    'device2_top_20_features.csv',
    'device3_top_20_features.csv',
    'device4_top_20_features.csv',
    'device5_top_20_features.csv',
    'device6_top_20_features.csv',
    'device7_top_20_features.csv',
    'device8_top_20_features.csv',
    'device9_top_20_features.csv'
]

# Loop through each CSV file
for i, csv_file in enumerate(csv_files):
    print(f"Processing file {i + 1} of 9: {csv_file}")

    # Load dataset from the current CSV file
    data = pd.read_csv(csv_file)

    # Separate features and labels
    X = data.drop(columns=['label'])
    y = data['label']

    # Define models
    random_forest_model = RandomForestClassifier()
    decision_tree_model = DecisionTreeClassifier()
    ada_model = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100)

    # Fit models to the data
    random_forest_model.fit(X, y)
    decision_tree_model.fit(X, y)
    ada_model.fit(X, y)

    # Calculate PFI-based feature importances for Random Forest
    pfi_result_rf = permutation_importance(random_forest_model, X, y, n_repeats=30, random_state=42)
    rf_feature_importances = pfi_result_rf.importances_mean

    # Calculate PFI-based feature importances for Decision Tree
    pfi_result_dt = permutation_importance(decision_tree_model, X, y, n_repeats=30, random_state=42)
    dt_feature_importances = pfi_result_dt.importances_mean

    # Calculate PFI-based feature importances for AdaBoost
    pfi_result_ada = permutation_importance(ada_model, X, y, n_repeats=30, random_state=42)
    ada_feature_importances = pfi_result_ada.importances_mean

    # Plot and save PFI-based feature importances for all models
    def save_feature_importance_plot(feature_importances, title, filename):
        plt.figure(figsize=(8, 6))
        plt.bar(range(len(X.columns)), feature_importances)
        plt.xticks(range(len(X.columns)), X.columns, rotation=90)
        plt.title(title)
        plt.tight_layout()
        plt.savefig(filename)
        plt.close()

    save_feature_importance_plot(rf_feature_importances, f'd{i + 1} Random Forest PFI Feature Importances', f'd{i + 1}_random_forest_pfi_feature_importance.jpg')
    save_feature_importance_plot(dt_feature_importances, f'd{i + 1} Decision Tree PFI Feature Importances', f'd{i + 1}_decision_tree_pfi_feature_importance.jpg')
    save_feature_importance_plot(ada_feature_importances, f'd{i + 1} AdaBoost PFI Feature Importances', f'd{i + 1}_ada_pfi_feature_importance.jpg')