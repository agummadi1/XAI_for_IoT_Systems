# -*- coding: utf-8 -*-
"""Task 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U6520Ay2YcUfTitMnpVWapnQ9xNaklHH

**Installing required Libraries**
"""

!pip install pandas numpy scikit-learn tensorflow shap

"""# **SHAP Global Summary Plot for each RPM for Mems Dataset**"""

import pandas as pd
import numpy as np
import tensorflow as tf
import shap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

def load_and_preprocess_data(filename):
    # Load the dataset
    mems_data = pd.read_csv(filename)

    # Data Cleaning
    imputer = SimpleImputer(strategy='mean')
    mems_data.fillna(mems_data.mean(), inplace=True)

    # Feature Selection
    selected_mems_features = ['x', 'y', 'z', 'label']
    mems_data = mems_data[selected_mems_features]

    # Data Splitting
    X = mems_data[['x', 'y', 'z']]
    y = mems_data['label']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalization (Standardization)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Encode the machine health condition labels
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.transform(y_test)

    return X_train, X_test, y_train_encoded, y_test_encoded, selected_mems_features

def build_and_train_model(X_train, y_train_encoded):
    # Build a simple Keras neural network model
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(3,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(3, activation='softmax')
    ])

    # Compile the model
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    model.fit(X_train, y_train_encoded, epochs=10, batch_size=32)

    return model

def shap_analysis(model, X_train, X_test, selected_mems_features):
    # SHAP Feature Importance Analysis using KernelExplainer
    explainer = shap.KernelExplainer(model.predict, X_train)
    shap_values = explainer.shap_values(X_test)

    # Rename the class labels
    class_labels = ['Normal', 'Near-failure', 'Failure']

    # Create a summary plot of SHAP feature importance with custom class labels
    shap.summary_plot(shap_values, X_test, feature_names=selected_mems_features[:-1], class_names=class_labels)

if __name__ == "__main__":
    # List of dataset filenames
    dataset_files = ['mem_100.csv', 'mem_200.csv', 'mem_300.csv',
                     'mem_400.csv', 'mem_500.csv', 'mem_600.csv']

    for dataset_file in dataset_files:
        X_train, X_test, y_train_encoded, y_test_encoded, selected_mems_features = load_and_preprocess_data(dataset_file)
        model = build_and_train_model(X_train, y_train_encoded)
        shap_analysis(model, X_train, X_test, selected_mems_features)

"""# **SHAP Global Summary Plot for each RPM for Piezoelectric Dataset**"""

import pandas as pd
import numpy as np
import tensorflow as tf
import shap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

def load_and_preprocess_data(filename):
    # Load the dataset
    piezo_data = pd.read_csv(filename)

    # Data Cleaning
    imputer = SimpleImputer(strategy='mean')
    piezo_data.fillna(piezo_data.mean(), inplace=True)

    # Feature Selection
    selected_piezo_features = ['x', 'y', 'z', 'label']
    piezo_data = piezo_data[selected_piezo_features]

    # Data Splitting
    X = piezo_data[['x', 'y', 'z']]
    y = piezo_data['label']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Normalization (Standardization)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Encode the machine health condition labels
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.transform(y_test)

    return X_train, X_test, y_train_encoded, y_test_encoded, selected_piezo_features

def build_and_train_model(X_train, y_train_encoded):
    # Build a simple Keras neural network model
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(3,)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(3, activation='softmax')
    ])

    # Compile the model
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    # Train the model
    model.fit(X_train, y_train_encoded, epochs=10, batch_size=32)

    return model

def shap_analysis(model, X_train, X_test, selected_piezo_features):
    # SHAP Feature Importance Analysis using KernelExplainer
    explainer = shap.KernelExplainer(model.predict, X_train)
    shap_values = explainer.shap_values(X_test)

    # Rename the class labels
    class_labels = ['Normal', 'Near-Failure', 'Failure']

    # Create a summary plot of SHAP feature importance with custom class labels
    shap.summary_plot(shap_values, X_test, feature_names=selected_piezo_features[:-1], class_names=class_labels)

if __name__ == "__main__":
    # List of dataset filenames for Piezoelectric Dataset
    dataset_files = ['piezo_100.csv', 'piezo_200.csv', 'piezo_300.csv',
                     'piezo_400.csv', 'piezo_500.csv', 'piezo_600.csv']

    for dataset_file in dataset_files:
        X_train, X_test, y_train_encoded, y_test_encoded, selected_piezo_features = load_and_preprocess_data(dataset_file)
        model = build_and_train_model(X_train, y_train_encoded)
        shap_analysis(model, X_train, X_test, selected_piezo_features)